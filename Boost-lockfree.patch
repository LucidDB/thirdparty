diff -urN boost-old/boost/lockfree/atomic_int.hpp boost/boost/lockfree/atomic_int.hpp
--- boost-old/boost/lockfree/atomic_int.hpp	1969-12-31 16:00:00.000000000 -0800
+++ boost/boost/lockfree/atomic_int.hpp	2009-06-30 16:00:02.000000000 -0700
@@ -0,0 +1,231 @@
+//  Copyright (C) 2007, 2008 Tim Blechmann & Thomas Grill
+//
+//  Distributed under the Boost Software License, Version 1.0. (See
+//  accompanying file LICENSE_1_0.txt or copy at
+//  http://www.boost.org/LICENSE_1_0.txt)
+
+//  Disclaimer: Not a Boost library.
+
+#ifndef BOOST_LOCKFREE_ATOMIC_INT_HPP
+#define BOOST_LOCKFREE_ATOMIC_INT_HPP
+
+#include <boost/lockfree/detail/prefix.hpp>
+#include <boost/lockfree/detail/cas.hpp>
+#include <boost/noncopyable.hpp>
+
+namespace boost
+{
+namespace lockfree
+{
+
+#if defined(__GNUC__) && ( (__GNUC__ > 4) || ((__GNUC__ >= 4) && (__GNUC_MINOR__ >= 1)) ) || __INTEL_COMPILER
+
+template <typename T>
+class atomic_int:
+    boost::noncopyable
+{
+public:
+    explicit atomic_int(T v = 0):
+        value(v)
+    {}
+
+    operator T(void) const
+    {
+        return __sync_fetch_and_add(&value, 0);
+    }
+
+    void operator =(T v)
+    {
+        value = v;
+        __sync_synchronize();
+    }
+
+    T operator +=(T v)
+    {
+        return __sync_add_and_fetch(&value, v);
+    }
+
+    T operator -=(T v)
+    {
+        return __sync_sub_and_fetch(&value, v);
+    }
+
+    /* prefix operator */
+    T operator ++(void)
+    {
+        return __sync_add_and_fetch(&value, 1);
+    }
+
+    /* prefix operator */
+    T operator --(void)
+    {
+        return __sync_sub_and_fetch(&value, 1);
+    }
+
+    /* postfix operator */
+    T operator ++(int)
+    {
+        return __sync_fetch_and_add(&value, 1);
+    }
+
+    /* postfix operator */
+    T operator --(int)
+    {
+        return __sync_fetch_and_sub(&value, 1);
+    }
+
+private:
+    mutable T value;
+};
+
+#elif defined(__GLIBCPP__) || defined(__GLIBCXX__)
+
+template <typename T>
+class atomic_int:
+    boost::noncopyable
+{
+public:
+    explicit atomic_int(T v = 0):
+        value(v)
+    {}
+
+    operator T(void) const
+    {
+        return __gnu_cxx::__exchange_and_add(&value, 0);
+    }
+
+    void operator =(T v)
+    {
+        value = v;
+    }
+
+    T operator +=(T v)
+    {
+        return __gnu_cxx::__exchange_and_add(&value, v) + v;
+    }
+
+    T operator -=(T v)
+    {
+        return __gnu_cxx::__exchange_and_add(&value, -v) - v;
+    }
+
+    /* prefix operator */
+    T operator ++(void)
+    {
+        return operator+=(1);
+    }
+
+    /* prefix operator */
+    T operator --(void)
+    {
+        return operator-=(1);
+    }
+
+    /* postfix operator */
+    T operator ++(int)
+    {
+        return __gnu_cxx::__exchange_and_add(&value, 1);
+    }
+
+    /* postfix operator */
+    T operator --(int)
+    {
+        return __gnu_cxx::__exchange_and_add(&value, -1);
+    }
+
+private:
+    mutable _Atomic_word value;
+};
+
+#else /* emulate via atomic_cas */
+
+template <typename T>
+class atomic_int:
+    boost::noncopyable
+{
+public:
+    explicit atomic_int(T v = 0)
+    {
+        *this = v;
+    }
+
+    operator T(void) const
+    {
+        memory_barrier();
+        return value;
+    }
+
+    void operator =(T v)
+    {
+        value = v;
+        memory_barrier();
+    }
+
+    /* prefix operator */
+    T operator ++()
+    {
+        return *this += 1;
+    }
+
+    /* prefix operator */
+    T operator --()
+    {
+        return *this -= 1;
+    }
+
+    T operator +=(T v)
+    {
+        for(;;)
+        {
+            T oldv = value;
+            T newv = oldv + v;
+            if(likely(atomic_cas(&value, oldv, newv)))
+                return newv;
+        }
+    }
+
+    T operator -=(T v)
+    {
+        for(;;)
+        {
+            T oldv = value;
+            T newv = oldv - v;
+
+            if(likely(atomic_cas(&value, oldv, newv)))
+                return newv;
+        }
+    }
+
+    /* postfix operator */
+    T operator ++(int)
+    {
+        for(;;)
+        {
+            T oldv = value;
+            if(likely(atomic_cas(&value, oldv, oldv+1)))
+                return oldv;
+        }
+    }
+
+    /* postfix operator */
+    T operator --(int)
+    {
+        for(;;)
+        {
+            T oldv = value;
+            if(likely(atomic_cas(&value, oldv, oldv-1)))
+                return oldv;
+        }
+    }
+
+private:
+    T value;
+};
+
+
+#endif
+
+} /* namespace lockfree */
+} /* namespace boost */
+
+#endif /* BOOST_LOCKFREE_ATOMIC_INT_HPP */
diff -urN boost-old/boost/lockfree/detail/branch_hints.hpp boost/boost/lockfree/detail/branch_hints.hpp
--- boost-old/boost/lockfree/detail/branch_hints.hpp	1969-12-31 16:00:00.000000000 -0800
+++ boost/boost/lockfree/detail/branch_hints.hpp	2009-06-30 16:00:02.000000000 -0700
@@ -0,0 +1,40 @@
+//  branch hints
+//  Copyright (C) 2007, 2008 Tim Blechmann
+//
+//  Distributed under the Boost Software License, Version 1.0. (See
+//  accompanying file LICENSE_1_0.txt or copy at
+//  http://www.boost.org/LICENSE_1_0.txt)
+
+//  Disclaimer: Not a Boost library.
+
+#ifndef BOOST_LOCKFREE_BRANCH_HINTS_HPP_INCLUDED
+#define BOOST_LOCKFREE_BRANCH_HINTS_HPP_INCLUDED
+
+namespace boost
+{
+namespace lockfree
+{
+    /** \brief hint for the branch prediction */
+    inline bool likely(bool expr)
+    {
+#ifdef __GNUC__
+        return __builtin_expect(expr, true);
+#else
+        return expr;
+#endif
+    }
+
+    /** \brief hint for the branch prediction */
+    inline bool unlikely(bool expr)
+    {
+#ifdef __GNUC__
+        return __builtin_expect(expr, false);
+#else
+        return expr;
+#endif
+    }
+
+} /* namespace lockfree */
+} /* namespace boost */
+
+#endif /* BOOST_LOCKFREE_BRANCH_HINTS_HPP_INCLUDED */
diff -urN boost-old/boost/lockfree/detail/cas.hpp boost/boost/lockfree/detail/cas.hpp
--- boost-old/boost/lockfree/detail/cas.hpp	1969-12-31 16:00:00.000000000 -0800
+++ boost/boost/lockfree/detail/cas.hpp	2009-06-30 16:00:02.000000000 -0700
@@ -0,0 +1,239 @@
+//  Copyright (C) 2007, 2008, 2009 Tim Blechmann & Thomas Grill
+//
+//  Distributed under the Boost Software License, Version 1.0. (See
+//  accompanying file LICENSE_1_0.txt or copy at
+//  http://www.boost.org/LICENSE_1_0.txt)
+
+//  Disclaimer: Not a Boost library.
+
+#ifndef BOOST_LOCKFREE_CAS_HPP_INCLUDED
+#define BOOST_LOCKFREE_CAS_HPP_INCLUDED
+
+#include <boost/lockfree/detail/prefix.hpp>
+#include <boost/detail/lightweight_mutex.hpp>
+#include <boost/static_assert.hpp>
+
+#include <boost/cstdint.hpp>
+
+namespace boost
+{
+namespace lockfree
+{
+
+inline void memory_barrier()
+{
+#if defined(__GNUC__) && ( (__GNUC__ > 4) || ((__GNUC__ >= 4) && (__GNUC_MINOR__ >= 1)) ) || defined(__INTEL_COMPILER)
+    __sync_synchronize();
+#elif defined(_MSC_VER) && (_MSC_VER >= 1300)
+    _ReadWriteBarrier();
+#elif defined(__APPLE__)
+    OSMemoryBarrier();
+#elif defined(AO_HAVE_nop_full)
+    AO_nop_full();
+#else
+#   warning "no memory barrier implemented for this platform"
+#endif
+}
+
+template <typename C>
+inline bool atomic_cas_emulation(volatile C * addr, C old, C nw)
+{
+    static boost::detail::lightweight_mutex guard;
+    boost::detail::lightweight_mutex::scoped_lock lock(guard);
+
+    if (*addr == old)
+    {
+        *addr = nw;
+        return true;
+    }
+    else
+        return false;
+}
+
+using boost::uint32_t;
+using boost::uint64_t;
+
+inline bool atomic_cas32(volatile uint32_t *  addr, uint32_t old, uint32_t nw)
+{
+#if defined(__GNUC__) && ( (__GNUC__ > 4) || ((__GNUC__ >= 4) && (__GNUC_MINOR__ >= 1)) ) || defined(__INTEL_COMPILER)
+    return __sync_bool_compare_and_swap(addr, old, nw);
+#else
+    return boost::interprocess::detail::atomic_cas32(addr, old, nw) == old;
+#endif
+}
+
+inline bool atomic_cas64(volatile uint64_t * addr, uint64_t old, uint64_t nw)
+{
+#if defined(__GNUC__) && ( (__GNUC__ > 4) || ((__GNUC__ >= 4) && (__GNUC_MINOR__ >= 1)) ) || defined(__INTEL_COMPILER)
+    return __sync_bool_compare_and_swap(addr, old, nw);
+#elif defined(_M_IX86)
+    return InterlockedCompareExchange(reinterpret_cast<volatile LONG*>(addr),
+                                      reinterpret_cast<LONG>(nw),
+                                      reinterpret_cast<LONG>(old)) == old;
+#elif defined(_M_X64)
+    return InterlockedCompareExchange(reinterpret_cast<volatile LONG*>(addr),
+                                      reinterpret_cast<LONG>(nw),
+                                      reinterpret_cast<LONG>(old)) == old;
+#else
+#warning ("blocking CAS emulation")
+    return atomic_cas_emulation(addr, old, nw);
+#endif
+}
+
+template <class C>
+inline bool atomic_cas(volatile C * addr, C old, C nw)
+{
+    if (sizeof(C) == 4)
+        return atomic_cas32((volatile uint32_t *)addr, (uint32_t)old, (uint32_t)nw);
+    else if (sizeof(C) == 8)
+        return atomic_cas64((volatile uint64_t *)addr, (uint64_t)old, (uint64_t)nw);
+    else
+        return atomic_cas_emulation(addr, old, nw);
+}
+
+
+template <class C, class D, class E>
+inline bool atomic_cas2(volatile C * addr, D old1, E old2, D new1, E new2)
+{
+#if defined(__GNUC__) && ((__GNUC__ >  4) || ( (__GNUC__ >= 4) && (__GNUC_MINOR__ >= 2) ) ) && defined(__i386__) && \
+    (defined(__i686__) || defined(__pentiumpro__) || defined(__nocona__ ) || \
+     defined (__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8))
+
+    struct packed_c
+    {
+        D d;
+        E e;
+    };
+
+    union cu
+    {
+        packed_c c;
+        long long l;
+    };
+
+    cu old;
+    old.c.d = old1;
+    old.c.e = old2;
+
+    cu nw;
+    nw.c.d = new1;
+    nw.c.e = new2;
+
+    return __sync_bool_compare_and_swap_8(reinterpret_cast<volatile long long*>(addr),
+                                          old.l,
+                                          nw.l);
+#elif defined(_M_IX86)
+    bool ok;
+    __asm {
+        mov eax,[old1]
+            mov edx,[old2]
+            mov ebx,[new1]
+            mov ecx,[new2]
+            mov edi,[addr]
+            lock cmpxchg8b [edi]
+            setz [ok]
+            }
+    return ok;
+#elif defined(__GNUC__) && (defined(__i686__) || defined(__pentiumpro__) || defined(__nocona__ ))
+    char result;
+#ifndef __PIC__
+    __asm__ __volatile__("lock; cmpxchg8b %0; setz %1"
+                         : "=m"(*addr), "=q"(result)
+                         : "m"(*addr), "d" (old1), "a" (old2),
+                           "c" (new1), "b" (new2) : "memory");
+#else
+    __asm__ __volatile__("push %%ebx; movl %6,%%ebx; lock; cmpxchg8b %0; setz %1; pop %%ebx"
+                         : "=m"(*addr), "=q"(result)
+                         : "m"(*addr), "d" (old1), "a" (old2),
+                           "c" (new1), "m" (new2) : "memory");
+#endif
+    return result != 0;
+#elif defined(AO_HAVE_double_compare_and_swap_full)
+    if (sizeof(D) != sizeof(AO_t) || sizeof(E) != sizeof(AO_t)) {
+        assert(false);
+        return false;
+    }
+
+    return AO_compare_double_and_swap_double_full(
+        reinterpret_cast<volatile AO_double_t*>(addr),
+        static_cast<AO_t>(old2),
+        reinterpret_cast<AO_t>(old1),
+        static_cast<AO_t>(new2),
+        reinterpret_cast<AO_t>(new1)
+        );
+
+#elif defined(__GNUC__) && defined(__x86_64__) &&                       \
+    ( __GCC_HAVE_SYNC_COMPARE_AND_SWAP_16 ) ||                          \
+    ( (__GNUC__ >  4) || ( (__GNUC__ >= 4) && (__GNUC_MINOR__ >= 2) ) && defined(__nocona__ ))
+
+    struct packed_c
+    {
+        long d;
+        long e;
+    };
+
+    typedef int TItype __attribute__ ((mode (TI)));
+
+    BOOST_STATIC_ASSERT(sizeof(packed_c) == sizeof(TItype));
+
+    union cu
+    {
+        packed_c c;
+        TItype l;
+    };
+
+    cu old;
+    old.c.d = (long)old1;
+    old.c.e = (long)old2;
+
+    cu nw;
+    nw.c.d = (long)new1;
+    nw.c.e = (long)new2;
+
+    return __sync_bool_compare_and_swap_16(reinterpret_cast<volatile TItype*>(addr),
+                                           old.l,
+                                           nw.l);
+
+#elif defined(__GNUC__) && defined(__x86_64__)
+    /* handcoded asm, will crash on early amd processors */
+    char result;
+    __asm__ __volatile__("lock; cmpxchg16b %0; setz %1"
+                         : "=m"(*addr), "=q"(result)
+                         : "m"(*addr), "d" (old2), "a" (old1),
+                           "c" (new2), "b" (new1) : "memory");
+    return result != 0;
+#else
+
+#ifdef _MSC_VER
+#pragma message ("blocking CAS2 emulation")
+#else
+#warning ("blocking CAS2 emulation")
+#endif
+
+    struct packed_c
+    {
+        D d;
+        E e;
+    };
+
+    volatile packed_c * packed_addr = reinterpret_cast<volatile packed_c*>(addr);
+
+    static boost::detail::lightweight_mutex guard;
+    boost::detail::lightweight_mutex::scoped_lock lock(guard);
+
+    if (packed_addr->d == old1 &&
+        packed_addr->e == old2)
+    {
+        packed_addr->d = new1;
+        packed_addr->e = new2;
+        return true;
+    }
+    else
+        return false;
+#endif
+}
+
+} /* namespace lockfree */
+} /* namespace boost */
+
+#endif /* BOOST_LOCKFREE_CAS_HPP_INCLUDED */
diff -urN boost-old/boost/lockfree/detail/freelist.hpp boost/boost/lockfree/detail/freelist.hpp
--- boost-old/boost/lockfree/detail/freelist.hpp	1969-12-31 16:00:00.000000000 -0800
+++ boost/boost/lockfree/detail/freelist.hpp	2009-06-30 16:00:02.000000000 -0700
@@ -0,0 +1,320 @@
+//  lock-free freelist
+//
+//  Copyright (C) 2008, 2009 Tim Blechmann
+//
+//  Distributed under the Boost Software License, Version 1.0. (See
+//  accompanying file LICENSE_1_0.txt or copy at
+//  http://www.boost.org/LICENSE_1_0.txt)
+
+//  Disclaimer: Not a Boost library.
+
+#ifndef BOOST_LOCKFREE_FREELIST_HPP_INCLUDED
+#define BOOST_LOCKFREE_FREELIST_HPP_INCLUDED
+
+#include <boost/lockfree/detail/tagged_ptr.hpp>
+#include <boost/lockfree/atomic_int.hpp>
+#include <boost/noncopyable.hpp>
+
+#include <boost/mpl/map.hpp>
+#include <boost/mpl/apply.hpp>
+#include <boost/mpl/at.hpp>
+#include <boost/type_traits/is_pod.hpp>
+
+#include <algorithm>            /* for std::min */
+
+namespace boost
+{
+namespace lockfree
+{
+namespace detail
+{
+
+template <typename T, typename Alloc = std::allocator<T> >
+class dummy_freelist:
+    private boost::noncopyable,
+    private Alloc
+{
+public:
+    T * allocate (void)
+    {
+        return Alloc::allocate(1);
+    }
+
+    void deallocate (T * n)
+    {
+        Alloc::deallocate(n, 1);
+    }
+};
+
+} /* namespace detail */
+
+/** simple freelist implementation  */
+template <typename T,
+          std::size_t maximum_size = 64,
+          typename Alloc = std::allocator<T> >
+class freelist:
+    private detail::dummy_freelist<T, Alloc>
+{
+    struct freelist_node
+    {
+        lockfree::tagged_ptr<freelist_node> next;
+    };
+
+    typedef lockfree::tagged_ptr<freelist_node> tagged_ptr;
+
+public:
+    freelist(void):
+        pool_(NULL)
+    {}
+
+    explicit freelist(std::size_t initial_nodes):
+        pool_(NULL)
+    {
+        for (std::size_t i = 0; i != std::min(initial_nodes, maximum_size); ++i)
+        {
+            T * node = detail::dummy_freelist<T, Alloc>::allocate();
+            deallocate(node);
+        }
+    }
+
+    ~freelist(void)
+    {
+        free_memory_pool();
+    }
+
+    T * allocate (void)
+    {
+        for(;;)
+        {
+            tagged_ptr old_pool(pool_);
+
+            if (!old_pool)
+                return detail::dummy_freelist<T, Alloc>::allocate();
+
+            freelist_node * new_pool = old_pool->next.get_ptr();
+
+            if (pool_.cas(old_pool, new_pool))
+            {
+                --free_list_size;
+                return reinterpret_cast<T*>(old_pool.get_ptr());
+            }
+        }
+    }
+
+    void deallocate (T * n)
+    {
+        if (free_list_size > maximum_size)
+        {
+            detail::dummy_freelist<T, Alloc>::deallocate(n);
+            return;
+        }
+
+        for(;;)
+        {
+            tagged_ptr old_pool (pool_);
+
+            freelist_node * new_pool = reinterpret_cast<freelist_node*>(n);
+
+            new_pool->next.set_ptr(old_pool.get_ptr());
+
+            if (pool_.cas(old_pool, new_pool))
+            {
+                --free_list_size;
+                return;
+            }
+        }
+    }
+
+private:
+    void free_memory_pool(void)
+    {
+        tagged_ptr current (pool_);
+
+        while (current)
+        {
+            freelist_node * n = current.get_ptr();
+            current.set(current->next);
+            detail::dummy_freelist<T, Alloc>::deallocate(reinterpret_cast<T*>(n));
+        }
+    }
+
+    tagged_ptr pool_;
+    atomic_int<unsigned long> free_list_size;
+};
+
+template <typename T, typename Alloc = std::allocator<T> >
+class caching_freelist:
+    private detail::dummy_freelist<T, Alloc>
+{
+    struct freelist_node
+    {
+        lockfree::tagged_ptr<freelist_node> next;
+    };
+
+    typedef lockfree::tagged_ptr<freelist_node> tagged_ptr;
+
+public:
+    caching_freelist(void):
+        pool_(NULL)
+    {}
+
+    explicit caching_freelist(std::size_t initial_nodes):
+        pool_(NULL)
+    {
+        for (std::size_t i = 0; i != initial_nodes; ++i)
+        {
+            T * node = detail::dummy_freelist<T, Alloc>::allocate();
+            deallocate(node);
+        }
+    }
+
+    ~caching_freelist(void)
+    {
+        free_memory_pool();
+    }
+
+    T * allocate (void)
+    {
+        for(;;)
+        {
+            tagged_ptr old_pool(pool_);
+
+            if (!old_pool)
+                return detail::dummy_freelist<T, Alloc>::allocate();
+
+            freelist_node * new_pool = old_pool->next.get_ptr();
+
+            if (pool_.cas(old_pool, new_pool))
+                return reinterpret_cast<T*>(old_pool.get_ptr());
+        }
+    }
+
+    void deallocate (T * n)
+    {
+        for(;;)
+        {
+            tagged_ptr old_pool (pool_);
+
+            freelist_node * new_pool = reinterpret_cast<freelist_node*>(n);
+
+            new_pool->next.set_ptr(old_pool.get_ptr());
+
+            if (pool_.cas(old_pool,new_pool))
+                return;
+        }
+    }
+
+private:
+    void free_memory_pool(void)
+    {
+        tagged_ptr current (pool_);
+
+        while (current)
+        {
+            freelist_node * n = current.get_ptr();
+            current.set(current->next);
+            detail::dummy_freelist<T, Alloc>::deallocate(reinterpret_cast<T*>(n));
+        }
+    }
+
+    tagged_ptr pool_;
+};
+
+template <typename T, typename Alloc = std::allocator<T> >
+class static_freelist:
+    private Alloc
+{
+    struct freelist_node
+    {
+        lockfree::tagged_ptr<freelist_node> next;
+    };
+
+    typedef lockfree::tagged_ptr<freelist_node> tagged_ptr;
+
+public:
+    explicit static_freelist(std::size_t max_nodes):
+        pool_(NULL), total_nodes(max_nodes)
+    {
+        chunks = Alloc::allocate(max_nodes);
+        for (std::size_t i = 0; i != max_nodes; ++i)
+        {
+            T * node = chunks + i;
+            deallocate(node);
+        }
+    }
+
+    ~static_freelist(void)
+    {
+        Alloc::deallocate(chunks, total_nodes);
+    }
+
+    T * allocate (void)
+    {
+        for(;;)
+        {
+            tagged_ptr old_pool(pool_);
+
+            if (!old_pool)
+                return 0;       /* allocation fails */
+
+            freelist_node * new_pool = old_pool->next.get_ptr();
+
+            if (pool_.cas(old_pool, new_pool))
+                return reinterpret_cast<T*>(old_pool.get_ptr());
+        }
+    }
+
+    void deallocate (T * n)
+    {
+        for(;;)
+        {
+            tagged_ptr old_pool (pool_);
+
+            freelist_node * new_pool = reinterpret_cast<freelist_node*>(n);
+
+            new_pool->next.set_ptr(old_pool.get_ptr());
+
+            if (pool_.cas(old_pool,new_pool))
+                return;
+        }
+    }
+
+private:
+    tagged_ptr pool_;
+
+    const std::size_t total_nodes;
+    T* chunks;
+};
+
+
+struct caching_freelist_t {};
+struct static_freelist_t {};
+
+namespace detail
+{
+
+#if 0
+template <typename T, typename Alloc, typename tag>
+struct select_freelist
+{
+private:
+    typedef typename Alloc::template rebind<T>::other Allocator;
+
+    typedef typename boost::lockfree::caching_freelist<T, Allocator> cfl;
+    typedef typename boost::lockfree::static_freelist<T, Allocator> sfl;
+
+    typedef typename boost::mpl::map<
+        boost::mpl::pair < caching_freelist_t, cfl/* typename boost::lockfree::caching_freelist<T, Alloc> */ >,
+        boost::mpl::pair < static_freelist_t,  sfl/* typename boost::lockfree::static_freelist<T, Alloc> */ >,
+        int
+        > freelists;
+public:
+    typedef typename boost::mpl::at<freelists, tag>::type type;
+};
+#endif
+
+} /* namespace detail */
+} /* namespace lockfree */
+} /* namespace boost */
+
+#endif /* BOOST_LOCKFREE_FREELIST_HPP_INCLUDED */
diff -urN boost-old/boost/lockfree/detail/prefix.hpp boost/boost/lockfree/detail/prefix.hpp
--- boost-old/boost/lockfree/detail/prefix.hpp	1969-12-31 16:00:00.000000000 -0800
+++ boost/boost/lockfree/detail/prefix.hpp	2009-06-30 16:00:02.000000000 -0700
@@ -0,0 +1,75 @@
+//  Copyright (C) 2007, 2008, 2009 Tim Blechmann & Thomas Grill
+//
+//  Distributed under the Boost Software License, Version 1.0. (See
+//  accompanying file LICENSE_1_0.txt or copy at
+//  http://www.boost.org/LICENSE_1_0.txt)
+
+//  Disclaimer: Not a Boost library.
+
+#ifndef BOOST_LOCKFREE_PREFIX_HPP_INCLUDED
+#define BOOST_LOCKFREE_PREFIX_HPP_INCLUDED
+
+#include <cassert>
+
+
+#ifdef _WIN32
+    #include <windows.h>
+#endif
+
+#ifdef __APPLE__
+    #include <libkern/OSAtomic.h>
+
+    #include <bits/atomicity.h>
+#endif
+
+#define BOOST_LOCKFREE_CACHELINE_BYTES 64
+
+#ifdef _MSC_VER
+// \note: Must use /Oi option for VC++ to enable intrinsics
+    extern "C" {
+        void __cdecl _ReadWriteBarrier();
+        LONG __cdecl _InterlockedCompareExchange(LONG volatile* Dest,LONG Exchange, LONG Comp);
+    }
+
+#define BOOST_LOCKFREE_CACHELINE_ALIGNMENT __declspec(align(BOOST_LOCKFREE_CACHELINE_BYTES))
+
+#if defined(_M_IX86)
+    #define BOOST_LOCKFREE_DCAS_ALIGNMENT
+#elif defined(_M_X64) || defined(_M_IA64)
+    #define BOOST_LOCKFREE_PTR_COMPRESSION 1
+    #define BOOST_LOCKFREE_DCAS_ALIGNMENT __declspec(align(16))
+#endif
+
+#endif /* _MSC_VER */
+
+#ifdef __GNUC__
+
+//#define BOOST_LOCKFREE_CACHELINE_ALIGNMENT __attribute__((aligned(BOOST_LOCKFREE_CACHELINE_ALIGNMENT)))
+#define BOOST_LOCKFREE_CACHELINE_ALIGNMENT __attribute__((aligned(64)))
+
+#ifdef __i386__
+    #define BOOST_LOCKFREE_DCAS_ALIGNMENT
+#elif defined(__ppc__)
+    #define BOOST_LOCKFREE_DCAS_ALIGNMENT
+#elif defined(__x86_64__)
+
+    #if !(defined (__GCC_HAVE_SYNC_COMPARE_AND_SWAP_16) || defined (__nocona__))
+        #define BOOST_LOCKFREE_PTR_COMPRESSION 1
+    #endif
+    #define BOOST_LOCKFREE_DCAS_ALIGNMENT __attribute__((aligned(16)))
+#endif
+
+#endif /* __GNUC__ */
+
+
+#ifdef USE_ATOMIC_OPS
+    #define AO_REQUIRE_CAS
+    #define AO_USE_PENTIUM4_INSTRS
+
+    extern "C" {
+        #include "../libatomic_ops/src/atomic_ops.h"
+    }
+#endif
+
+
+#endif /* BOOST_LOCKFREE_PREFIX_HPP_INCLUDED */
diff -urN boost-old/boost/lockfree/detail/tagged_ptr_dcas.hpp boost/boost/lockfree/detail/tagged_ptr_dcas.hpp
--- boost-old/boost/lockfree/detail/tagged_ptr_dcas.hpp	1969-12-31 16:00:00.000000000 -0800
+++ boost/boost/lockfree/detail/tagged_ptr_dcas.hpp	2009-06-30 16:00:02.000000000 -0700
@@ -0,0 +1,168 @@
+//  tagged pointer, for aba prevention
+//
+//  Copyright (C) 2008 Tim Blechmann
+//
+//  Distributed under the Boost Software License, Version 1.0. (See
+//  accompanying file LICENSE_1_0.txt or copy at
+//  http://www.boost.org/LICENSE_1_0.txt)
+
+//  Disclaimer: Not a Boost library.
+
+#ifndef BOOST_LOCKFREE_TAGGED_PTR_DCAS_HPP_INCLUDED
+#define BOOST_LOCKFREE_TAGGED_PTR_DCAS_HPP_INCLUDED
+
+#include <boost/lockfree/detail/cas.hpp>
+#include <boost/lockfree/detail/branch_hints.hpp>
+
+#include <cstddef>              /* for std::size_t */
+
+namespace boost
+{
+namespace lockfree
+{
+
+template <class T>
+class BOOST_LOCKFREE_DCAS_ALIGNMENT tagged_ptr
+{
+public:
+    typedef std::size_t tag_t;
+
+    /** uninitialized constructor */
+    tagged_ptr(void)//: ptr(0), tag(0)
+    {}
+
+    /** copy constructor */
+    tagged_ptr(tagged_ptr const & p)//: ptr(0), tag(0)
+    {
+        set(p);
+    }
+
+    explicit tagged_ptr(T * p, tag_t t = 0):
+        ptr(p), tag(t)
+    {}
+
+    /** atomic set operations */
+    /* @{ */
+    void operator= (tagged_ptr const & p)
+    {
+        atomic_set(p);
+    }
+
+    void atomic_set(tagged_ptr const & p)
+    {
+        for (;;)
+        {
+            tagged_ptr old;
+            old.set(*this);
+            if(likely(cas(old, p.ptr, p.tag)))
+                return;
+        }
+    }
+
+    void atomic_set(T * p, tag_t t)
+    {
+        for (;;)
+        {
+            tagged_ptr old;
+            old.set(*this);
+
+            if(likely(cas(old, p, t)))
+                return;
+        }
+    }
+    /* @} */
+
+    /** unsafe set operation */
+    /* @{ */
+    void set(tagged_ptr const & p)
+    {
+        ptr = p.ptr;
+        tag = p.tag;
+    }
+
+    void set(T * p, tag_t t)
+    {
+        ptr = p;
+        tag = t;
+    }
+    /* @} */
+
+    /** comparing semantics */
+    /* @{ */
+    bool operator== (tagged_ptr const & p) const
+    {
+        return (ptr == p.ptr) && (tag == p.tag);
+    }
+
+    bool operator!= (tagged_ptr const & p) const
+    {
+        return !operator==(p);
+    }
+    /* @} */
+
+    /** pointer access */
+    /* @{ */
+    T * get_ptr() const
+    {
+        return ptr;
+    }
+
+    void set_ptr(T * p)
+    {
+        ptr = p;
+    }
+    /* @} */
+
+    /** tag access */
+    /* @{ */
+    tag_t get_tag() const
+    {
+        return tag;
+    }
+
+    void set_tag(tag_t t)
+    {
+        tag = t;
+    }
+    /* @} */
+
+    /** compare and swap  */
+    /* @{ */
+    bool cas(tagged_ptr const & oldval, T * newptr)
+    {
+        return boost::lockfree::atomic_cas2(this, oldval.ptr, oldval.tag, newptr, oldval.tag + 1);
+    }
+
+    bool cas(tagged_ptr const & oldval, T * newptr, tag_t t)
+    {
+        return boost::lockfree::atomic_cas2(this, oldval.ptr, oldval.tag, newptr, t);
+    }
+    /* @} */
+
+    /** smart pointer support  */
+    /* @{ */
+    T & operator*() const
+    {
+        return *ptr;
+    }
+
+    T * operator->() const
+    {
+        return ptr;
+    }
+
+    operator bool(void) const
+    {
+        return ptr != 0;
+    }
+    /* @} */
+
+protected:
+    T * ptr;
+    tag_t tag;
+};
+
+} /* namespace lockfree */
+} /* namespace boost */
+
+#endif /* BOOST_LOCKFREE_TAGGED_PTR_DCAS_HPP_INCLUDED */
diff -urN boost-old/boost/lockfree/detail/tagged_ptr.hpp boost/boost/lockfree/detail/tagged_ptr.hpp
--- boost-old/boost/lockfree/detail/tagged_ptr.hpp	1969-12-31 16:00:00.000000000 -0800
+++ boost/boost/lockfree/detail/tagged_ptr.hpp	2009-06-30 16:00:02.000000000 -0700
@@ -0,0 +1,22 @@
+//  tagged pointer, for aba prevention
+//
+//  Copyright (C) 2008 Tim Blechmann
+//
+//  Distributed under the Boost Software License, Version 1.0. (See
+//  accompanying file LICENSE_1_0.txt or copy at
+//  http://www.boost.org/LICENSE_1_0.txt)
+
+//  Disclaimer: Not a Boost library.
+
+#ifndef BOOST_LOCKFREE_TAGGED_PTR_HPP_INCLUDED
+#define BOOST_LOCKFREE_TAGGED_PTR_HPP_INCLUDED
+
+#include <boost/lockfree/detail/prefix.hpp>
+
+#ifndef BOOST_LOCKFREE_PTR_COMPRESSION
+#include <boost/lockfree/detail/tagged_ptr_dcas.hpp>
+#else
+#include <boost/lockfree/detail/tagged_ptr_ptrcompression.hpp>
+#endif
+
+#endif /* BOOST_LOCKFREE_TAGGED_PTR_HPP_INCLUDED */
diff -urN boost-old/boost/lockfree/detail/tagged_ptr_ptrcompression.hpp boost/boost/lockfree/detail/tagged_ptr_ptrcompression.hpp
--- boost-old/boost/lockfree/detail/tagged_ptr_ptrcompression.hpp	1969-12-31 16:00:00.000000000 -0800
+++ boost/boost/lockfree/detail/tagged_ptr_ptrcompression.hpp	2009-06-30 16:00:02.000000000 -0700
@@ -0,0 +1,201 @@
+//  tagged pointer, for aba prevention
+//
+//  Copyright (C) 2008, 2009 Tim Blechmann, based on code by Cory Nelson
+//
+//  Distributed under the Boost Software License, Version 1.0. (See
+//  accompanying file LICENSE_1_0.txt or copy at
+//  http://www.boost.org/LICENSE_1_0.txt)
+
+//  Disclaimer: Not a Boost library.
+
+#ifndef BOOST_LOCKFREE_TAGGED_PTR_PTRCOMPRESSION_HPP_INCLUDED
+#define BOOST_LOCKFREE_TAGGED_PTR_PTRCOMPRESSION_HPP_INCLUDED
+
+#include <boost/lockfree/detail/cas.hpp>
+#include <boost/lockfree/detail/branch_hints.hpp>
+
+#include <cstddef>              /* for std::size_t */
+
+#include <boost/cstdint.hpp>
+
+namespace boost
+{
+namespace lockfree
+{
+
+#if defined (__x86_64__) || defined (_M_X64)
+
+template <class T>
+class BOOST_LOCKFREE_DCAS_ALIGNMENT tagged_ptr
+{
+    typedef boost::uint64_t compressed_ptr_t;
+    typedef boost::uint16_t tag_t;
+
+private:
+    union cast_unit
+    {
+        compressed_ptr_t value;
+        tag_t tag[4];
+    };
+
+    static const int tag_index = 3;
+    static const compressed_ptr_t ptr_mask = 0xffffffffffff; //(1L<<48L)-1;
+
+    static T* extract_ptr(compressed_ptr_t const & i)
+    {
+        return (T*)(i & ptr_mask);
+    }
+
+    static tag_t extract_tag(compressed_ptr_t const & i)
+    {
+        cast_unit cu;
+        cu.value = i;
+        return cu.tag[tag_index];
+    }
+
+    static compressed_ptr_t pack_ptr(T * ptr, int tag)
+    {
+        cast_unit ret;
+        ret.value = compressed_ptr_t(ptr);
+        ret.tag[tag_index] = tag;
+        return ret.value;
+    }
+
+public:
+    /** uninitialized constructor */
+    tagged_ptr(void)//: ptr(0), tag(0)
+    {}
+
+    /** copy constructor */
+    tagged_ptr(tagged_ptr const & p)//: ptr(0), tag(0)
+    {
+        set(p);
+    }
+
+    explicit tagged_ptr(T * p, tag_t t = 0):
+        ptr(pack_ptr(p, t))
+    {}
+
+    /** atomic set operations */
+    /* @{ */
+    void operator= (tagged_ptr const & p)
+    {
+        atomic_set(p);
+    }
+
+    void atomic_set(tagged_ptr const & p)
+    {
+        set(p);
+    }
+
+    void atomic_set(T * p, tag_t t)
+    {
+        ptr = pack_ptr(p, t);
+    }
+    /* @} */
+
+    /** unsafe set operation */
+    /* @{ */
+    void set(tagged_ptr const & p)
+    {
+        ptr = p.ptr;
+    }
+
+    void set(T * p, tag_t t)
+    {
+        ptr = pack_ptr(p, t);
+    }
+    /* @} */
+
+    /** comparing semantics */
+    /* @{ */
+    bool operator== (tagged_ptr const & p) const
+    {
+        return (ptr == p.ptr);
+    }
+
+    bool operator!= (tagged_ptr const & p) const
+    {
+        return !operator==(p);
+    }
+    /* @} */
+
+    /** pointer access */
+    /* @{ */
+    T * get_ptr() const
+    {
+        return extract_ptr(ptr);
+    }
+
+    void set_ptr(T * p)
+    {
+        tag_t tag = get_tag();
+        ptr = pack_ptr(p, tag);
+    }
+    /* @} */
+
+    /** tag access */
+    /* @{ */
+    tag_t get_tag() const
+    {
+        return extract_tag(ptr);
+    }
+
+    void set_tag(tag_t t)
+    {
+        T * p = get_ptr();
+        ptr = pack_ptr(p, t);
+    }
+    /* @} */
+
+    /** compare and swap  */
+    /* @{ */
+private:
+    bool cas(compressed_ptr_t const & oldval, compressed_ptr_t const & newval)
+    {
+        return boost::lockfree::atomic_cas(&(this->ptr), oldval, newval);
+    }
+
+public:
+    bool cas(tagged_ptr const & oldval, T * newptr)
+    {
+        compressed_ptr_t new_compressed_ptr = pack_ptr(newptr, extract_tag(oldval.ptr)+1);
+        return cas(oldval.ptr, new_compressed_ptr);
+    }
+
+    bool cas(tagged_ptr const & oldval, T * newptr, tag_t t)
+    {
+        compressed_ptr_t new_compressed_ptr = pack_ptr(newptr, t);
+        return boost::lockfree::atomic_cas(&(this->ptr), oldval.ptr, new_compressed_ptr);
+    }
+    /* @} */
+
+    /** smart pointer support  */
+    /* @{ */
+    T & operator*() const
+    {
+        return *get_ptr();
+    }
+
+    T * operator->() const
+    {
+        return get_ptr();
+    }
+
+    operator bool(void) const
+    {
+        return get_ptr() != 0;
+    }
+    /* @} */
+
+protected:
+    compressed_ptr_t ptr;
+};
+#else
+#error unsupported platform
+#endif
+
+} /* namespace lockfree */
+} /* namespace boost */
+
+#endif /* BOOST_LOCKFREE_TAGGED_PTR_PTRCOMPRESSION_HPP_INCLUDED */
diff -urN boost-old/boost/lockfree/fifo.hpp boost/boost/lockfree/fifo.hpp
--- boost-old/boost/lockfree/fifo.hpp	1969-12-31 16:00:00.000000000 -0800
+++ boost/boost/lockfree/fifo.hpp	2009-06-30 16:00:02.000000000 -0700
@@ -0,0 +1,273 @@
+//  lock-free fifo queue from
+//  Michael, M. M. and Scott, M. L.,
+//  "simple, fast and practical non-blocking and blocking concurrent queue algorithms"
+//
+//  implementation for c++
+//
+//  Copyright (C) 2008 Tim Blechmann
+//
+//  Distributed under the Boost Software License, Version 1.0. (See
+//  accompanying file LICENSE_1_0.txt or copy at
+//  http://www.boost.org/LICENSE_1_0.txt)
+
+//  Disclaimer: Not a Boost library.
+
+#ifndef BOOST_LOCKFREE_FIFO_HPP_INCLUDED
+#define BOOST_LOCKFREE_FIFO_HPP_INCLUDED
+
+#include <boost/lockfree/atomic_int.hpp>
+#include <boost/lockfree/detail/tagged_ptr.hpp>
+#include <boost/lockfree/detail/freelist.hpp>
+
+#include <boost/static_assert.hpp>
+#include <boost/type_traits/is_pod.hpp>
+
+#include <memory>               /* std::auto_ptr */
+#include <boost/scoped_ptr.hpp>
+#include <boost/shared_ptr.hpp>
+#include <boost/noncopyable.hpp>
+
+namespace boost
+{
+namespace lockfree
+{
+
+namespace detail
+{
+
+template <typename T, typename freelist_t, typename Alloc>
+class fifo:
+    boost::noncopyable
+{
+    BOOST_STATIC_ASSERT(boost::is_pod<T>::value);
+
+    struct BOOST_LOCKFREE_CACHELINE_ALIGNMENT node
+    {
+        node(T const & v):
+            data(v)
+        {
+            next.set(NULL, next.get_tag()+1); /* increment tag to avoid ABA problem */
+        }
+
+        node (void):
+            next(NULL)
+        {}
+
+        tagged_ptr<node> next;
+        T data;
+    };
+
+    typedef tagged_ptr<node> atomic_node_ptr;
+
+    typedef typename Alloc::template rebind<node>::other node_allocator;
+/*     typedef typename select_freelist<node, node_allocator, freelist_t>::type pool_t; */
+
+    typedef typename boost::mpl::if_<boost::is_same<freelist_t, caching_freelist_t>,
+                                     caching_freelist<node, node_allocator>,
+                                     static_freelist<node, node_allocator>
+                                     >::type pool_t;
+
+public:
+    fifo(void):
+        pool(128)
+    {
+        node * n = alloc_node();
+        head_.set_ptr(n);
+        tail_.set_ptr(n);
+    }
+
+    explicit fifo(std::size_t initial_nodes):
+        pool(initial_nodes)
+    {
+        node * n = alloc_node();
+        head_.set_ptr(n);
+        tail_.set_ptr(n);
+    }
+
+    ~fifo(void)
+    {
+        assert(empty());
+        dealloc_node(head_.get_ptr());
+    }
+
+    bool empty(void)
+    {
+        return head_.get_ptr() == tail_.get_ptr();
+    }
+
+    bool enqueue(T const & t)
+    {
+        node * n = alloc_node(t);
+
+        if (n == NULL)
+            return false;
+
+        for (;;)
+        {
+            atomic_node_ptr tail (tail_);
+            memory_barrier();
+            atomic_node_ptr next (tail->next);
+            memory_barrier();
+
+            if (likely(tail == tail_))
+            {
+                if (next.get_ptr() == 0)
+                {
+                    if ( tail->next.cas(next, n) )
+                    {
+                        tail_.cas(tail, n);
+                        return true;
+                    }
+                }
+                else
+                    tail_.cas(tail, next.get_ptr());
+            }
+        }
+    }
+
+    bool dequeue (T * ret)
+    {
+        for (;;)
+        {
+            atomic_node_ptr head (head_);
+            memory_barrier();
+
+            atomic_node_ptr tail(tail_);
+            node * next = head->next.get_ptr();
+            memory_barrier();
+
+            if (likely(head == head_))
+            {
+                if (head.get_ptr() == tail.get_ptr())
+                {
+                    if (next == 0)
+                        return false;
+
+                    tail_.cas(tail, next);
+                }
+                else
+                {
+                    *ret = next->data;
+                    if (head_.cas(head, next))
+                    {
+                        dealloc_node(head.get_ptr());
+
+                        return true;
+                    }
+                }
+            }
+        }
+    }
+
+private:
+    node * alloc_node(void)
+    {
+        node * chunk = pool.allocate();
+        new(chunk) node();
+        return chunk;
+    }
+
+    node * alloc_node(T const & t)
+    {
+        node * chunk = pool.allocate();
+        new(chunk) node(t);
+        return chunk;
+    }
+
+    void dealloc_node(node * n)
+    {
+        n->~node();
+        pool.deallocate(n);
+    }
+
+    atomic_node_ptr head_;
+    atomic_node_ptr BOOST_LOCKFREE_CACHELINE_ALIGNMENT tail_; /* force head_ and tail_ to different cache lines! */
+
+    pool_t pool;
+};
+
+} /* namespace detail */
+
+/** lockfree fifo
+ *
+ *  - wrapper for detail::fifo
+ * */
+template <typename T,
+          typename freelist_t = caching_freelist_t,
+          typename Alloc = std::allocator<T>
+          >
+class fifo:
+    public detail::fifo<T, freelist_t, Alloc>
+{
+public:
+    fifo(void)
+    {}
+
+    explicit fifo(std::size_t initial_nodes):
+        detail::fifo<T, freelist_t, Alloc>(initial_nodes)
+    {}
+};
+
+
+/** lockfree fifo, template specialization for pointer-types
+ *
+ *  - wrapper for detail::fifo
+ *  - overload dequeue to support smart pointers
+ * */
+template <typename T, typename freelist_t, typename Alloc>
+class fifo<T*, freelist_t, Alloc>:
+    public detail::fifo<T*, freelist_t, Alloc>
+{
+    typedef detail::fifo<T*, freelist_t, Alloc> fifo_t;
+
+    template <typename smart_ptr>
+    bool dequeue_smart_ptr(smart_ptr & ptr)
+    {
+        T * result = 0;
+        bool success = fifo_t::dequeue(&result);
+
+        if (success)
+            ptr.reset(result);
+        return success;
+    }
+
+public:
+    fifo(void)
+    {}
+
+    explicit fifo(std::size_t initial_nodes):
+        fifo_t(initial_nodes)
+    {}
+
+    bool enqueue(T * t)
+    {
+        return fifo_t::enqueue(t);
+    }
+
+    bool dequeue (T ** ret)
+    {
+        return fifo_t::dequeue(ret);
+    }
+
+    bool dequeue (std::auto_ptr<T> & ret)
+    {
+        return dequeue_smart_ptr(ret);
+    }
+
+    bool dequeue (boost::scoped_ptr<T> & ret)
+    {
+        BOOST_STATIC_ASSERT(sizeof(boost::scoped_ptr<T>) == sizeof(T*));
+        return dequeue(reinterpret_cast<T**>(&ret));
+    }
+
+    bool dequeue (boost::shared_ptr<T> & ret)
+    {
+        return dequeue_smart_ptr(ret);
+    }
+};
+
+} /* namespace lockfree */
+} /* namespace boost */
+
+
+#endif /* BOOST_LOCKFREE_FIFO_HPP_INCLUDED */
diff -urN boost-old/boost/lockfree/stack.hpp boost/boost/lockfree/stack.hpp
--- boost-old/boost/lockfree/stack.hpp	1969-12-31 16:00:00.000000000 -0800
+++ boost/boost/lockfree/stack.hpp	2009-06-30 16:00:02.000000000 -0700
@@ -0,0 +1,128 @@
+//  Copyright (C) 2008 Tim Blechmann
+//
+//  Distributed under the Boost Software License, Version 1.0. (See
+//  accompanying file LICENSE_1_0.txt or copy at
+//  http://www.boost.org/LICENSE_1_0.txt)
+
+//  Disclaimer: Not a Boost library.
+
+#ifndef BOOST_LOCKFREE_STACK_HPP_INCLUDED
+#define BOOST_LOCKFREE_STACK_HPP_INCLUDED
+
+#include <boost/checked_delete.hpp>
+
+#include <boost/static_assert.hpp>
+#include <boost/type_traits/is_base_of.hpp>
+
+#include <boost/lockfree/detail/tagged_ptr.hpp>
+#include <boost/lockfree/detail/freelist.hpp>
+#include <boost/noncopyable.hpp>
+
+
+namespace boost
+{
+namespace lockfree
+{
+template <typename T,
+          typename freelist_t = caching_freelist_t,
+          typename Alloc = std::allocator<T>
+          >
+class stack:
+    boost::noncopyable
+{
+    struct node
+    {
+        node(T const & v):
+            v(v)
+        {}
+
+        tagged_ptr<node> next;
+        T v;
+    };
+
+    typedef tagged_ptr<node> ptr_type;
+
+    typedef typename Alloc::template rebind<node>::other node_allocator;
+/*     typedef typename detail::select_freelist<node, node_allocator, freelist_t>::type pool_t; */
+
+    typedef typename boost::mpl::if_<boost::is_same<freelist_t, caching_freelist_t>,
+                                     caching_freelist<node, node_allocator>,
+                                     static_freelist<node, node_allocator>
+                                     >::type pool_t;
+
+public:
+    stack(void):
+        tos(NULL), pool(128)
+    {}
+
+    explicit stack(std::size_t n):
+        tos(NULL), pool(n)
+    {}
+
+    bool push(T const & v)
+    {
+        node * newnode = alloc_node(v);
+
+        if (newnode == 0)
+            return false;
+
+        ptr_type old_tos;
+        do
+        {
+            old_tos.set(tos);
+            newnode->next.set_ptr(old_tos.get_ptr());
+        }
+        while (!tos.cas(old_tos, newnode));
+
+        return true;
+    }
+
+    bool pop(T * ret)
+    {
+        for (;;)
+        {
+            ptr_type old_tos;
+            old_tos.set(tos);
+
+            if (!old_tos)
+                return false;
+
+            node * new_tos = old_tos->next.get_ptr();
+
+            if (tos.cas(old_tos, new_tos))
+            {
+                *ret = old_tos->v;
+                dealloc_node(old_tos.get_ptr());
+                return true;
+            }
+        }
+    }
+
+    bool empty(void) const
+    {
+        return tos == NULL;
+    }
+
+private:
+    node * alloc_node(T const & t)
+    {
+        node * chunk = pool.allocate();
+        new(chunk) node(t);
+        return chunk;
+    }
+
+    void dealloc_node(node * n)
+    {
+        n->~node();
+        pool.deallocate(n);
+    }
+
+    ptr_type tos;
+    pool_t pool;
+};
+
+
+} /* namespace lockfree */
+} /* namespace boost */
+
+#endif /* BOOST_LOCKFREE_STACK_HPP_INCLUDED */
